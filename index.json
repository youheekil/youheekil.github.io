[{"categories":["Machine Learning"],"content":" Machine learning is nothing but a geometry problem ! All data is the same. Understanding geometry is very important to solve machine learning problems. ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:0:0","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":"[Numpy] Basic Linear Algebra Linear algebra, mathematical discipline that deals with vectors and matrices and, more generally, with vector spaces and linear transformations (Britannica) In this lecture, you will learn Scala, Vector, Array, Tensor Dot product \u0026 Norm Multiplication \u0026 Transpose \u0026 Invertible matrix Linear Transformation Eigen Value \u0026 Eigen Vector Cosine Similarity and perform these with Numpy ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:1:0","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":"1. Scala, Vector, Matrix, Tensor Scala A Scalar has only magnitude (size) and is like a number a = 1 b = 0.25 Vector Vector A vector has magnitude and direction and is a list of numbers (can be in a row or column) which could present as $ \\vec{p} = (1, 2, 3)$ import numpy as np a = np.array([1, 2, 3]) b = np.array([2, -2.4, 0.5]) Matrix A matrix is an array of numbers import numpy as np a = np.array([[1, 2, 3], [4, 5, 6]]) # 2 X 3 matrix b = np.array([[2, -2.4, 0.5, 0.14], [1, 0.3, 2, 1]]) # 2 X 4 matrix Tensor Assuming a basis of a real vector space, e.g., a coordinate frame in the ambient space,, a tensor can be represented as an organized multidimensional array of scalars (numerical values) with respect to this specific basis - wikipedia Visualization of scalar, vector, array, and tensor ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:1:1","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":"2. Dot product \u0026 Norm Dot product Dot product equals to inner product which is the sum of the products of corresponding components. Numpy helps us calculate vector’s inner product easily with dot() function. Inner Product Reference Check Wolfram MathWorld page to get a deeper understanding of inner product! a = np.array([1, 2, 3]) b = np.array([3, 2, 1]) print(np.dot(a, b)) # inner product by using dot() print(np.sum(a * b)) # inner product by using summation of multiplication The vector norm refers to the length of the vector. In machine learning, the most commonly used norms are $L^2$ Norm and $L^1$ Norm. $L^2$ Norm $L^2$ is represented as $||\\vec{x}||_2$. $$||\\vec{x}||_2 = \\sqrt{x_1^2 + x_2^2+ \\cdot \\cdot \\cdot + x_n^2} = \\sqrt{\\sum^n_k{x^2_k}}$$ where $k = 1…n$. $L^1$ Norm $L^1$ is represented as $||\\vec{x}||_1$. $$||\\vec{x}||_1 = |x_1| + |x_2| + … + |x_n| = \\sum^n_k{|x_k|}$$. In Numpy, Norm can be calculated by using linalg.norm() function. a = np.array([1, 1, -2, -2]) # L2 Norm (default) np.linalg.norm(a) # L1 Norm np.linalg.norm(a, 1) ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:1:2","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":"3. Multiplication \u0026 Transpose \u0026 Invertible matrix Multiplication Numpy’s dot() function can be used for matrix multiplication # a = 2 X 3 a = np.array([[1, 2, 3], [1, 2, 3]]) # b = 3 X 2 b = np.array([[1, 2], [3, 4], [5, 6]]) print(np.dot(a, b)) # 2 x 2 Transpose The transpose of a matrix is simply a flipped version of the original matrix. We can transpose a matrix by switching its rows with its columns. In numpy, implementing matrix transpose can be done by just adding .T at the end of the original matrix a = np.array([[1, 2, 3], [1, 2, 3]]) print(a.T) # Transpose of a matrix A Multiplying matrix A and transpose of a matrix A can be implemented as following: a = np.array([[1, 2, 3], [1, 2, 3]]) print(np.dot(a, a.T)) Invertible matrix In linear algebra, an n-by-n square matrix is called invertible, if the product of the matrix and its inverse is the identity matrix (CUEMATH) $$ AB = BA = I_n$$ $$\\implies B = A^{-1}$$ where, A is (n x n) invertible matrix B is (n x n) matrix called inverse of A $I_n$ is (n x n) identity matrix a = np.array([[1, 2], [3, 4]]) print(np.linalg.det(a)) # case when Determinant (det(A)) is not zero -\u003e invertible matrix exists b = np.array([[1, 2], [1, 2]]) print(np.linalg.det(b)) # case when Determinant (det(B)) is zero -\u003e invertible matrix doesn't exist Implement Invertible matrix a = np.array([[1, 2], [3, 4]]) print(np.linalg.inv(a)) — working on it ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:1:3","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":"4. Linear Transformation Linear Transformations are often used in machine learning applications. ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:1:4","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":"5. Eigen Value \u0026 Eigen Vector ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:1:5","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":"6. Cosine Similarity ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt2/:1:6","tags":null,"title":"[Machine Learning] Basic Math for ML with Python","uri":"/machine-learning-basic-math_pt2/"},{"categories":["Machine Learning"],"content":" Machine learning is nothing but a geometry problem ! All data is the same. Understanding geometry is very important to solve machine learning problems. ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt1/:0:0","tags":null,"title":"[Machine Learning] Basic Math for ML with Python (Part 1)","uri":"/machine-learning-basic-math_pt1/"},{"categories":["Machine Learning"],"content":"[Numpy] Basic Linear Algebra Linear algebra, mathematical discipline that deals with vectors and matrices and, more generally, with vector spaces and linear transformations (Britannica) In this lecture, you will learn Scala, Vector, Array, Tensor Dot product \u0026 Norm Multiplication \u0026 Transpose \u0026 Invertible matrix Linear Transformation Eigen Value \u0026 Eigen Vector Cosine Similarity and perform these with Numpy ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt1/:1:0","tags":null,"title":"[Machine Learning] Basic Math for ML with Python (Part 1)","uri":"/machine-learning-basic-math_pt1/"},{"categories":["Machine Learning"],"content":"1. Scala, Vector, Matrix, Tensor Scala A Scalar has only magnitude (size) and is like a number a = 1 b = 0.25 Vector Vector A vector has magnitude and direction and is a list of numbers (can be in a row or column) which could present as $ \\vec{p} = (1, 2, 3)$ import numpy as np a = np.array([1, 2, 3]) b = np.array([2, -2.4, 0.5]) Matrix A matrix is an array of numbers import numpy as np a = np.array([[1, 2, 3], [4, 5, 6]]) # 2 X 3 matrix b = np.array([[2, -2.4, 0.5, 0.14], [1, 0.3, 2, 1]]) # 2 X 4 matrix Tensor Assuming a basis of a real vector space, e.g., a coordinate frame in the ambient space,, a tensor can be represented as an organized multidimensional array of scalars (numerical values) with respect to this specific basis - wikipedia Visualization of scalar, vector, array, and tensor ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt1/:1:1","tags":null,"title":"[Machine Learning] Basic Math for ML with Python (Part 1)","uri":"/machine-learning-basic-math_pt1/"},{"categories":["Machine Learning"],"content":"2. Dot product \u0026 Norm Dot product Dot product equals to inner product which is the sum of the products of corresponding components. Numpy helps us calculate vector’s inner product easily with dot() function. Inner Product Reference Check Wolfram MathWorld page to get a deeper understanding of inner product! a = np.array([1, 2, 3]) b = np.array([3, 2, 1]) print(np.dot(a, b)) # inner product by using dot() print(np.sum(a * b)) # inner product by using summation of multiplication The vector norm refers to the length of the vector. In machine learning, the most commonly used norms are $L^2$ Norm and $L^1$ Norm. $L^2$ Norm $L^2$ is represented as $||\\vec{x}||_2$. $$||\\vec{x}||_2 = \\sqrt{x_1^2 + x_2^2+ \\cdot \\cdot \\cdot + x_n^2} = \\sqrt{\\sum^n_k{x^2_k}}$$ where $k = 1…n$. $L^1$ Norm $L^1$ is represented as $||\\vec{x}||_1$. $$||\\vec{x}||_1 = |x_1| + |x_2| + … + |x_n| = \\sum^n_k{|x_k|}$$. In Numpy, Norm can be calculated by using linalg.norm() function. a = np.array([1, 1, -2, -2]) # L2 Norm (default) np.linalg.norm(a) # L1 Norm np.linalg.norm(a, 1) ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt1/:1:2","tags":null,"title":"[Machine Learning] Basic Math for ML with Python (Part 1)","uri":"/machine-learning-basic-math_pt1/"},{"categories":["Machine Learning"],"content":"3. Multiplication \u0026 Transpose \u0026 Invertible matrix Multiplication Numpy’s dot() function can be used for matrix multiplication # a = 2 X 3 a = np.array([[1, 2, 3], [1, 2, 3]]) # b = 3 X 2 b = np.array([[1, 2], [3, 4], [5, 6]]) print(np.dot(a, b)) # 2 x 2 Transpose The transpose of a matrix is simply a flipped version of the original matrix. We can transpose a matrix by switching its rows with its columns. In numpy, implementing matrix transpose can be done by just adding .T at the end of the original matrix a = np.array([[1, 2, 3], [1, 2, 3]]) print(a.T) # Transpose of a matrix A Multiplying matrix A and transpose of a matrix A can be implemented as following: a = np.array([[1, 2, 3], [1, 2, 3]]) print(np.dot(a, a.T)) Invertible matrix In linear algebra, an n-by-n square matrix is called invertible, if the product of the matrix and its inverse is the identity matrix (CUEMATH) $$ AB = BA = I_n$$ $$\\implies B = A^{-1}$$ where, A is (n x n) invertible matrix B is (n x n) matrix called inverse of A $I_n$ is (n x n) identity matrix a = np.array([[1, 2], [3, 4]]) print(np.linalg.det(a)) # case when Determinant (det(A)) is not zero -\u003e invertible matrix exists b = np.array([[1, 2], [1, 2]]) print(np.linalg.det(b)) # case when Determinant (det(B)) is zero -\u003e invertible matrix doesn't exist Implement Invertible matrix a = np.array([[1, 2], [3, 4]]) print(np.linalg.inv(a)) ","date":"2022-07-26","objectID":"/machine-learning-basic-math_pt1/:1:3","tags":null,"title":"[Machine Learning] Basic Math for ML with Python (Part 1)","uri":"/machine-learning-basic-math_pt1/"},{"categories":["Data Engineer"],"content":"What is RFM? RFM is a technique that can perform Customer Segmentation to determine customer’s buying behavior. Company uses the RFM metric as a customer behavior segmentation indicator to improve marketing strategies for revenue increases by reactivating customers to more royal. R(Recency) : The last time the customer made a purchase. The smaller the number, the better. F(Frequency) : Number of transactions. The bigger the number, the better M(Monetary) : The spending power of a customer. The bigger the number, the better. ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:1:0","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"Data points used in RFM Analysis Recency - last order date Frequency - count of total orders Monetary value - total spend ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:2:0","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"Understand Data (SQL) Check the whole data SELECT * FROM sales_data Inspecting Data SELECT DISTINCT status FROM sales_data -- NICE TO PLOT SELECT DISTINCT year_id FROM sales_data SELECT DISTINCT PRODUCTLINE FROM sales_data -- NICE TO PLOT SELECT DISTINCT COUNTRY FROM sales_data -- NICE TO PLOT SELECT DISTINCT DEALSIZE FROM sales_data -- NICE TO PLOT SELECT DISTINCT TERRITORY FROM sales_data -- NICE TO PLOT SELECT DISTINCT MONTH_ID FROM sales_data WHERE year_id = 2003 ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:3:0","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"Analysis ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:4:0","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"1. Salse By Product SELECT PRODUCTLINE, SUM(sales) AS revenue FROM sales_data GROUP BY PRODUCTLINE ORDER BY 2 DESC; By year SELECT YEAR_ID, SUM(sales) AS revenue FROM sales_data GROUP BY YEAR_ID ORDER BY 2 DESC; By Dealsize SELECT DEALSIZE, SUM(sales) AS revenue FROM sales_data GROUP BY DEALSIZE ORDER BY 2 DESC; ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:4:1","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"2. Best Month for sales per month SELECT MONTH_ID, SUM(sales) AS Revenue, COUNT(ORDERNUMBER) AS Frequency FROM sales_data WHERE YEAR_ID = 2003 -- CAN CHANGE YEAR TO 2004 GROUP BY MONTH_ID ORDER BY Revenue DESC; SELECT MONTH_ID, SUM(sales) AS Revenue, COUNT(ORDERNUMBER) AS Frequency FROM sales_data WHERE YEAR_ID = 2003 AND MONTH_ID = 11 -- CAN CHANGE YEAR TO 2004 GROUP BY MONTH_ID, PRODUCTLINE ORDER BY Revenue DESC; ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:4:2","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"3. WHO IS OUR BEST CUSTOMER (this could be answered with RFM) SELECT CUSTOMERNAME, SUM(sales) AS MonetaryValue, AVG(sales) AS AvgMonetaryValue, COUNT(ORDERNUMBER) AS Frequency, MAX(ORDERDATE) AS last_order_date, DATEDIFF(DD, MAX(ORDERDATE), (SELECT MAX(ORDERDATE) FROM sales_data AS max_order_date)) AS Recency FROM sales_data GROUP BY CUSTOMERNAME ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:4:3","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"Categorize the RFM WITH rfm AS ( SELECT CUSTOMERNAME, SUM(sales) AS MonetaryValue, AVG(sales) AS AvgMonetaryValue, COUNT(ORDERNUMBER) AS Frequency, MAX(ORDERDATE) AS last_order_date, DATEDIFF(DD, MAX(ORDERDATE), (SELECT MAX(ORDERDATE) FROM sales_data AS max_order_date)) AS Recency FROM sales_data GROUP BY CUSTOMERNAME ) rfm_calc AS ( SELECT R.*, NTILE(4) OVER (ORDER BY Recency) rfm_recency, NTILE(4) OVER (ORDER BY Frequency) rfm_frequency, NTILE(4) OVER (ORDER BY AvgMonetaryValue) rfm_monetary, FROM rfm R ORDER BY 4 DESC ) SELECT C.*, CAST(rfm_recency AS VARCHAR) + CAST(rfm_frequency AS VARCHAR) + CAST(rfm_monetary AS VARCHAR) AS rfm_cell_string FROM rfm_calc C ; ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:4:4","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"Create TEMP Table DROP TABLE IF EXISTS #rfm --for global table ##rfm WITH rfm AS ( SELECT CUSTOMERNAME, SUM(sales) AS MonetaryValue, AVG(sales) AS AvgMonetaryValue, COUNT(ORDERNUMBER) AS Frequency, MAX(ORDERDATE) AS last_order_date, DATEDIFF(DD, MAX(ORDERDATE), (SELECT MAX(ORDERDATE) FROM sales_data AS max_order_date)) AS Recency FROM sales_data GROUP BY CUSTOMERNAME ), rfm_calc AS ( SELECT R.*, NTILE(4) OVER (ORDER BY Recency) rfm_recency, NTILE(4) OVER (ORDER BY Frequency) rfm_frequency, NTILE(4) OVER (ORDER BY MonetaryValue) rfm_monetary, FROM rfm R ORDER BY 4 DESC ) SELECT C.*, CAST(rfm_recency AS VARCHAR) + CAST(rfm_frequency AS VARCHAR) + CAST(rfm_monetary AS VARCHAR) AS rfm_cell_string into #rfm FROM rfm_calc C ; SELECT CUSTOMERNAME, rfm_recency, rfm_freqeuncy, rfm_monetary, CASE WHEN rfm_cell_string IN (111, 112, 121, 122, 123, 132, 211, 212, 114, 141) THEN 'lost_customers' WHEN rfm_cell_string IN (133, 134, 143, 244, 334, 343, 344) THEN 'slipping away, cannot lose' WHEN rfm_cell_string IN (311, 411, 331) THEN 'new_customers' WHEN rfm_cell_string IN (222, 223, 233, 322) THEN 'potential churners' WHEN rfm_cell_string IN (323, 333, 321, 422, 332, 432) THEN 'active' WHEN rfm_cell_string IN (433, 434, 443, 444) THEN 'loyal' FROM #rfm ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:4:5","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Data Engineer"],"content":"What products codes are sold together? SELECT DISTINCT ORDERNUMBER, STUFF( (SELECT ',' + PRODUCTCODE FROM sales_data P WHERE ORDERNUMBER IN ( SELECT ORDERNUMBER FROM ( SELECT ORDERNUMBER, COUNT(*) AS rn, FROM sales_data WHERE STATUS='Shipped' GROUP BY ORDERNUMBER ) m where rn = 2 -- CAN CHANGE THE NUMBER ) AND p.ORDERNUMBER = s.ORDERNUMBER for xml path ('')), 1, 1, '') FROM sales_data S ORDER BY 2 DESC ","date":"2022-07-15","objectID":"/data-analysis-with-power-bi/:4:6","tags":null,"title":"RFM Analysis with SQL \u0026 Power BI (DAX)","uri":"/data-analysis-with-power-bi/"},{"categories":["Project"],"content":"Setup with Confluent Kafka, Spark, Delta Lake with Databricks and AWS Project Final Diagram ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:0:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Instruction In this post, we will set up environment to perform Starbucks Twitter Sentiment Analysis with Confluent Kafka, Spark, Delta Lake with Databricks and AWS. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:1:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 1. Twitter API Credentials As we performed in the previous post, we need to get Twitter API Credentials. After getting it, we save these credential information in .env file. Make sure to include .env file in .gitignore to be ignored in the future. # .env CONSUMER_KEY = \"\u003capi key\u003e\" CONSUMER_SECRET = \"\u003capi secret\u003e\" ACCESS_TOKEN_KEY = \"\u003caccess key\u003e\" ACCESS_TOKEN_SECRET = \"\u003caccess secret\u003e\" ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:2:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 2. Confluent Cloud Confluent Cloud is a resilient, scalable streaming data service based on Apache Kafka®, delivered as a fully managed service - Confluent Cloud. It offers users to manage cluster resources easily. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:3:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"2-1. Create a Confluent Cloud account and Kafka cluster First, create a free Confluent Cloud account and create a kafka cluster in Confluent Cloud. I created a basic cluster which supports single zone availability with aws cloud provider. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:3:1","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"2-2. Create a Kafka Topic named tweet_data with 2 partitions. From the navigation menu, click Topics, and in the Topics page, click Create topic. I set topic name as tweet_data with 2 partitions, the topic created on the Kafka cluster will be available for use by producers and consumers. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:3:2","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 3. Confluent Cloud API credentials. API keys From the navigation menu, click API keys under Data Integration. If there is no available API Keys, click add key to get a new API keys (API_KEY, API_SECRET) and make sure to save it somewhere safe. HOST: Bootstrap server From the navigation menu, click Cluster settings under Cluster Overview. You can find Identification block which contains the information of Bootstrap server. Make sure to save it somewhere safe. It should be similar to pkc-w12qj.ap-southeast-1.aws.confluent.cloud:9092 HOST = pkc-w12qj.ap-southeast-1.aws.confluent.cloud Save those at $HOME/.confluent/python.config vi $HOME/.confluent/python.config Press i and copy\u0026paste the file below ! #kafka bootstrap.servers={HOST}:9092 security.protocol=SASL_SSL sasl.mechanisms=PLAIN sasl.username={API_KEY} sasl.password={API_SECRET} Then, replace HOST, API_KEY, API_SECRET with the values from Step 3. Press :wq to save the file. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:4:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 4. Create a Databricks Cluster In this post, we are going to deploy Databricks on the AWS. Instruction to create a Databricks Cluster on AWS is well explained in HERE. Click the compute under navigator bar, create a Create Cluster, and add some configuration like below in the picture. Create a Databricks Cluster After creating a Databricks Cluster, it’s time to explore the Databricks Workspace. Click the Workspace under navigator bar. Click the users, \u003cuser-account\u003e, then create a Notebook`. Create a Databricks Workspace Once you are done with creating the Databricks Notebook, please check the my github page for the source code of twitter data ingestion. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:5:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 4-1. Install Dependencies When you are creating a Cluster, you can find the libraries tab next next to Configuration tab. If you need any dependencies needed in the future, you can use this to install. Or you can install dependencies like this, %pip install delta-spark spark-nlp==3.3.3 wordcloud contractions gensim pyldavis==3.2.0 too. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:6:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 5. Source Code for twitter data ingestion Required files for twitter real-time data ingestion Check the source codes in my github page producer/producer.py producer/ccloud_lib.py run.sh Dockerfile .env requirements.txt Still some modifications are needed # DockerfileFROMpython:3.7-slimCOPY requirements.txt /tmp/requirements.txtRUN pip3 install -U -r /tmp/requirements.txtCOPY producer/ /producerCMD [ \"python3\", \"producer/producer.py\", \"-f\", \"/root/.confluent/librdkafka.config\", \"-t\", \"\u003cyour-kafka-topic-name\u003e\" ] Procedure to run the kafka twitter data ingestion pip install virtualenv virtualenv --version # test your installation cd \u003cyour-project_folder\u003e virtualenv ccloud-venv source ./ccloud-venv/bin/activate bash run.sh ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:7:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 6. Spark Streaming in Databricks - Streaming Data Ingestion Add Confluent API Credentials as we used before in Step 3 and copy and paste the code below for readStreaming Kafka data in the Workspace we created. confluentApiKey = \"xxx\" confluentSecret = \"xxx\" host = \"xxx\" streamingInputDF = spark \\ .readStream \\ .format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", host) \\ .option(\"kafka.security.protocol\", \"SASL_SSL\") \\ .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(confluentApiKey, confluentSecret)) \\ .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\") \\ .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\ .option(\"startingOffsets\", \"latest\") \\ .option(\"failOnDataLoss\", \"false\") \\ .option(\"subscribe\", \"product\") \\ .load() In order to run the Workspace, we need to attach the cluster we created before. ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:8:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 7. Spark Streaming in Databricks - Streaming Data Transformation Please check the whole procedure of streaming data tranformation in notebooks/twitter_ingestion_transformation.ipynb in my github-pages! ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:9:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 8. Connect DataBricks and Delta Lake We are able to build a complete streaming data pipeline to consolidate the data by using Confluent Kafka as an input/source system for Spark Structured Streaming and Delta Lake as a storage layer. Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It helps unify streaming and batch data processing. A Delta Lake table is both a batch table as well as a streaming source and sink. As data are stored in Parquet files, delta lake is storage agnostic. It could be an Amazon S3 bucket or an Azure Data Lake Storage container - Michelen Blog. %pip install delta-spark from delta import * ... # after doing whole data transformation # Save the sentiment tweets streaming data into delta Lake under the path, /tmp/delta-tweet-table sentiment_tweet = sentiment_tweets \\ .writeStream.format(\"delta\") \\ .outputMode(\"append\") \\ .trigger(processingTime='10 seconds') \\ .option(\"checkpointLocation\", \"/tmp/checkpoint\") \\ .start(\"/tmp/delta-tweet-table\") # reading data in Delta Lake DF = ( spark.read \\ .format(\"delta\") \\ .load(\"/tmp/delta-tweet-table\") \\ .createOrReplaceTempView(\"table2\") ) display(spark.sql(\"SELECT * FROM table2 LIMIT 1000\")) ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:10:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Step 9. Spark NLP %pip install spark-nlp Installing Spark NLP from PyPI is not enough to run Spark NLP in Databricks. Therefore, we still need to install a dependency - spark-nlp_2.12:3.4.4 (something similar to this one) under the libraries tab in the Cluster. Or attch spark-nlp-1.3.0.jar to the cluster. This library can be downloaded from the spark-packages repository https://spark-packages.org/package/JohnSnowLabs/spark-nlp. from pyspark.ml import Pipeline # spark-nlp-1.3.0.jar is attached to the cluster. This library was downloaded from the # spark-packages repository https://spark-packages.org/package/JohnSnowLabs/spark-nlp from sparknlp.annotator import * from sparknlp.common import * from sparknlp.base import * ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:11:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Create a Preprocessing Stages Pipeline # Create pre-processing stages # Stage 1: DocumentAssembler as entry point documentAssembler = DocumentAssembler() \\ .setInputCol(\"value\") \\ .setOutputCol(\"document\") # Stage 2: Tokenizer tokenizer = Tokenizer() \\ .setInputCols([\"document\"]) \\ .setOutputCol(\"token\") # Stage 3: Normalizer to lower text and to remove html tags, hyperlinks, twitter handles, # alphanumeric characters (integers, floats), timestamps in the format hh:mm (e.g. 10:30) and punctuation cleanUpPatterns = [r\"RT\", \"\u003c[^\u003e]*\u003e\", r\"www\\S+\", r\"http\\S+\", \"@[^\\s]+\", \"[\\d-]\", \"\\d*\\.\\d+\", \"\\d*\\:\\d+\", \"[^\\w\\d\\s]\"] normalizer = Normalizer() \\ .setInputCols(\"token\") \\ .setOutputCol(\"normalized\") \\ .setCleanupPatterns(cleanUpPatterns) \\ .setLowercase(True) # Stage 4: Remove stopwords stopwords = StopWordsCleaner()\\ .setInputCols(\"normalized\")\\ .setOutputCol(\"cleanTokens\")\\ .setCaseSensitive(False) # Stage 5: Lemmatizer lemma = LemmatizerModel.pretrained() \\ .setInputCols([\"cleanTokens\"]) \\ .setOutputCol(\"lemma\") # Stage 6: Stemmer stems tokens to bring it to root form #.setInputCols([\"cleanTokens\"]).setOutputCol(\"stem\") \\ stemmer = Stemmer() \\ .setInputCols([\"lemma\"]) \\ .setInputCols([\"cleanTokens\"]) \\ .setOutputCol(\"stem\") # Stage 7: Finisher to convert custom document structure to array of tokens finisher = Finisher() \\ .setInputCols([\"stem\"]) \\ .setOutputCols([\"token_features\"]) \\ .setOutputAsArray(True) \\ .setCleanAnnotations(False) from pyspark.ml import Pipeline # Check pre-processing pipeline prep_pipeline = Pipeline(stages=[documentAssembler, tokenizer, normalizer, stopwords, lemma, stemmer, finisher]) empty_df = spark.createDataFrame([['']]).toDF(\"value\") prep_pipeline_model = prep_pipeline.fit(empty_df) result = prep_pipeline_model.transform(tweet_df) The pipeline is followed by the procedure as below. NLP pipeline in Spark ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:11:1","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"[Option] integrate databricks notebook with Github You can connect databricks notebook with Github for the revision history. The procedure is described in here ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:12:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Jupyter Notebook ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:13:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project"],"content":"Reference https://github.com/scoyne2/kafka_spark_streams https://blogit.michelin.io/kafka-to-delta-lake-using-apache-spark-streaming-avro/ https://medium.com/@lorenagongang/sentiment-analysis-on-streaming-twitter-data-using-kafka-spark-structured-streaming-python-part-b27aecca697a https://winf-hsos.github.io/databricks-notebooks/big-data-analytics/ss-2020/Word%20Clouds%20with%20Python.html ","date":"2022-06-08","objectID":"/project1-1.-starbucks-twitter-sentimental-analysis/:14:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Instructions and Spark NLP","uri":"/project1-1.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Architecture Planning From Kafka to Delta Lake using Apache Spark Structured Streaming Image Source: From Kafka to Delta Lake using Apache Spark Structured Streaming ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:0:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"1. Aim The aim of the Starbucks Twitter Sentimental Analysis project is to build end-to-end twitter data streaming pipeline to analyze brand sentiment analysis. Brand sentiment analysis is, to put it simply, a way of determining the general attitude toward your brand, product, or service. Nowadays, the easiest way to analyze brand sentiment is through media monitoring tools. Customers often express their feeling or option toward the brand on social media channels. These opinions will give insight on the brand’s position. The sentimental is determined mostly just as positive, negative, or neutral (MediaToolKit) Project Overview ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:1:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"2. Happy Path connect with big data streams to ingest twtiter data. Script Planning producer.py consumer.py config (.env) Dockerfile run.sh ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:2:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"3. Tech Stack Kafka, Spark Streaming, Confluent Cloud, Databricks, Delta Lake, Spark NLP ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:3:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"4. Procedure There were three options to choose for this project. The first Option I considered was the combination of Kafka and Spark streaming. The second option was the combination of socket and Spark streaming, and third is the combination of Confluent Cloud and Databricks. ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:4:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Option 1. Docker Platform Twitter - Kafka - Spark Streaming ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:4:1","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Option 2 local environment Twitter - Socket - Spark Streaming ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:4:2","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Option 3. Combination of Confluent Cloud and Databricks Twitter - Kafka - Spark Streaming After tons of trials\u0026errors and concerns, I decided to go for Option 3. There are few reasons, first, I faced some struggles while submiting python file for spark streaming. My local environment kept giving me errors whenever I tried to submit the python file with spark-submit command. Second, I wanted to try different cloud platform, and found out that Confluent Cloud performs reall well. For the Spark Streaming, Databricks was used since it is well organized and easy to setup for Spark Streaming and have Delta Lake. ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:4:3","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"5. What is Spark Streaming? Data is continuously flowing in so that it’s a bit different than working with batch data. Processing engine to process data in real-time from sources and output data to external storage systems. STREAMING API is diffrent than API calls. For Streaming API, we just make one request to API that opens up portal where all of the event/data are going to be streaming back to client. ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:5:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Spark Streaming vs Spark Structured Streaming ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:6:0","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Identify Input sources File Source: reads files written in a directory as a stream of data. Supported file formats are text, CSV, JSON, ORC, Parquet kafka Source: Reads data from Kafka. Socket Source(for testing): Reads UTF8 text data from a socket connection. The listening server socket is at the driver. ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:6:1","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Identify Output Modes Append Mode: Spark will output only newly processed row since the last trigger Update Mode: Spark will output only updated rows since the last trigger Complete mode: Spark will output all the rows it has processed so far. ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:6:2","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Output Sinks File sink: Store the output to a directory Kafka sink: Store the output to one or more topics in Kafka Console sink (for debugging): The output is printed to the console/stdout every time there is a trigger Memory sink (for debugging): The output is sotred in memory as an in-memory table ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:6:3","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Testing locally with sockets Testing with sockets helps you to design the overall data extraction, transformation and storage place in your local environment. Once you scale up, other services such as Kafka can replace the socket. Socket is like an electric socket. Data is coming into the socket like electricity and then Spark is the plug that accepts that electricity and sends it to your data stroage place. ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:6:4","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Project","Sentiment-Analysis"],"content":"Real Examples of data streaming Chat bots Message apps Real-time ML prediction like Netflix, Amazon E-commerce apps ","date":"2022-06-06","objectID":"/project1-0.-starbucks-twitter-sentimental-analysis/:6:5","tags":["Sentiment-Analysis","kafka","spark-streaming"],"title":"[Starbucks Twitter Sentiment Analysis] Architecture Planning","uri":"/project1-0.-starbucks-twitter-sentimental-analysis/"},{"categories":["Sentiment Analysis"],"content":"TextBlob The TextBlob method produces polarity and subjectivity score. The polarity score which falls between [-1.0, 1.0] indicates a sensentivity from the sentence. If the score is below zero (0.0), sensitivity of the sentence is negativity. While the score is above zero (0.0), then the sensitivity of the sentence is positive. The subjectivity score which falls between [0.0, 1.0] identifies whether the sentence is objective or subjectivity. If the score is close to 0.0, the sentence tends to be more objective. On the other hand, if the score is close to 1.0, the sentence tends to be more subjective. # import libraries import pandas as pd import seaborn as sns from textblob import TextBlob simple_sentence = \"Business Process Management systems (BPMS) are a rich source of events that document the execution of processes and activities within these systems.\" # singularize blob = TextBlob(simple_sentence.lower()) # model TextBlob(simple_sentence).sentiment # words print(\"words: {}\".format(blob.words)) # sentiment print(TextBlob(simple_sentence).polarity) ","date":"2022-06-05","objectID":"/text-analysis/:1:0","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"pyspark.sql ","date":"2022-06-05","objectID":"/text-analysis/:2:0","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"1. Create a SparkSession First, we need to create a SparkSession which serves as an entry point to Spark SQL. from pyspark.sql import SparkSession sc = SparkSession.builder.getOrCreate() sc.SparkContext.setLogLevel(\"WARN\") print(sc) ","date":"2022-06-05","objectID":"/text-analysis/:2:1","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"2. Create a spark dataframe by reading a csv file df = sc.read.option(\"header\", \"true\").csv(\u003cpath-to-the-csvfile\u003e) df.columns ","date":"2022-06-05","objectID":"/text-analysis/:2:2","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"3. Select The select function helps to create a subset of the data df.select(\"column1\", \"column2\").show(5) ","date":"2022-06-05","objectID":"/text-analysis/:2:3","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"4. filter from pyspark.sql import functions as F df.filter( (F.col('column1') == \"condition1\") \u0026 (F.col('column2') \u003e 100) ).count() ","date":"2022-06-05","objectID":"/text-analysis/:2:4","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"5. withColumn The withColumn function is very useful, it can be used to manipulate columns or create new columns. df = df.withColumn('new_column', F.col('column2')/100) df.select('new_column').show(5) ","date":"2022-06-05","objectID":"/text-analysis/:2:5","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"6. GroupBy The groupby function is also very useful for data analysis which does grouping data points (rows). df.groupby('column1').agg( F.mean('new_column').alias('column3') ).show() The alias function does assigning a new name to the aggregated column ","date":"2022-06-05","objectID":"/text-analysis/:2:6","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"orderby The orderby function is used for sorting the values. df.groupby(\"column1\").agg( F.round(F.mean(\"new_column\"), 2).alias(\"column3\") ).orderBy( F.col(\"column3\"), descending=False ).show() ","date":"2022-06-05","objectID":"/text-analysis/:2:7","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"lit The lit function is to create a column by assigning a literal or constant value. new_df = df.filter(F.col('Type') == 'h').select( 'column1', 'column2', 'column3' ).withColumn('is_true', F.lit(1)) # assign 'is_true' column to 1(true) new_df.show(4) ","date":"2022-06-05","objectID":"/text-analysis/:2:8","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"when The when function evaluates the given conditions and returns values accordingly. df.select(when(df['column1']=='condition1', 1)\\ .otherwise(0)\\ .alias('is_true'))\\ .show() ","date":"2022-06-05","objectID":"/text-analysis/:2:9","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"pyspark.sql.functions ","date":"2022-06-05","objectID":"/text-analysis/:3:0","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"get_json_object The function get_json_object() is used to extract the JSON string based on path from the JSON column from pyspark.sql.functions import get_json_object df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\ .show(truncate=False) //+---+-----------+ //|id |ZipCodeType| //+---+-----------+ //|1 |STANDARD | //+---+-----------+ ","date":"2022-06-05","objectID":"/text-analysis/:3:1","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"UDFs - User-Defined Functions User-Defined Functions (UDFs) are user-programmable routines that act on one row. from textblob import TextBlob # define get_sentiment function def get_sentiment(text): try: tweet = TextBlob(text) return tweet.sentiment.polarity except: return None # define your function from pyspark.sql.functions import UserDefinedFunction getSentiment = UserDefinedFunction(get_sentiment, StringType()) ## apply the UDF using withColumn df.selectExpr(\"cast(data as string)\")\\ .withColumn('tweet', get_json_object(col('data'),\"$[0].tweet\"))\\ .withColumn('sentiment', getSentiment(col('tweet')).cast(FloatType())) ","date":"2022-06-05","objectID":"/text-analysis/:3:2","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"pyspark.streaming Return a new DStream by applying incremental reduceByKey over a sliding window. DStream.reduceByKeyAndWindow(func, invFun, windowDuration, slideDuration=None, numPartitions=None, filterFunc=None) ","date":"2022-06-05","objectID":"/text-analysis/:4:0","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Sentiment Analysis"],"content":"Reference https://towardsdatascience.com/7-must-know-pyspark-functions-d514ca9376b9 ","date":"2022-06-05","objectID":"/text-analysis/:5:0","tags":null,"title":"Sentiment Analysis with NLTK, TextBlob, Spark Streaming","uri":"/text-analysis/"},{"categories":["Project"],"content":"There are two basic recommender systems: (1) Collaborative Filtering, (2) Content-Based Filtering. It differs by what kinds of data that you are working with. Collaborative Filtering approach works with the user-item interactions types of data, such as ratings or buying behavior. On the other hand, Content-Based Filtering approach is for the attribute information about the users and items, such as textual profiles or relevant keywords. In this post, I am going to perform an effective song recommendataion system with the combination of two user’s informations - mood and favorite artist. ","date":"2022-05-23","objectID":"/project2-2.-songplz-bot/:0:0","tags":null,"title":"[SongPlz-Bot] 2. Severless \u0026 Data Ingestion \u0026 Recommender System","uri":"/project2-2.-songplz-bot/"},{"categories":["Project"],"content":"Recommender system architecture First, each user will get questions like below asking user’s mood by color and favorite singer. Question Messages from a SongPlz bot in Slack PHASE 1 : React to commands in Slack channel, do some basic operations like retrieving global top 10 songs. PHASE 2 : ","date":"2022-05-23","objectID":"/project2-2.-songplz-bot/:1:0","tags":null,"title":"[SongPlz-Bot] 2. Severless \u0026 Data Ingestion \u0026 Recommender System","uri":"/project2-2.-songplz-bot/"},{"categories":["Project"],"content":"Mood For the song recommendataion algorithm based on user’s mood, I took some references from researchgate. Defination of Mood by color 2D Arousal-Valence 2D plane So, few colors will be listed to let users choose depending on their mood (red 🔴, yellow 🟡, navy blue 🔵, purple 🟣, white ⚪). Also, Spotify provides audio feature for each song which contains value of Danceability, Energy, Instrumentalness, Liveness, Loudness, Speechiness, Tempo, Valence from 0.0 to 1.0. In this post, two audio features - energy and valence - will be used mainly. Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry) (Bo Plantinga). After we collected the information related to mood \u0026 color, and researched available information we can get from Spotify, we came up with the final recommendation approach to recommend songs based on user’s information. Final song recommendation approach planning based on user's mood ","date":"2022-05-23","objectID":"/project2-2.-songplz-bot/:1:1","tags":null,"title":"[SongPlz-Bot] 2. Severless \u0026 Data Ingestion \u0026 Recommender System","uri":"/project2-2.-songplz-bot/"},{"categories":["Project"],"content":"Singer Artists and Track Data ETL pipeline is required. We calculate average of audio features of each artists’s top 10 songs that user would like to listen by using either Euclidean Distance or Cosine Distance. Save top 3 most similar artists into postgreSQL or MySQL. ","date":"2022-05-23","objectID":"/project2-2.-songplz-bot/:1:2","tags":null,"title":"[SongPlz-Bot] 2. Severless \u0026 Data Ingestion \u0026 Recommender System","uri":"/project2-2.-songplz-bot/"},{"categories":["Project"],"content":"Data Modelling There are many more Response status code as you can check HERE. Artist Column Data Type Artist Id VARCHAR(256) Artist Name VARCHAR(256) Artist Genre VARCHAR(256) Followers INT(11) Popularity INT(11) Artist Uri VARCHAR(256) Artist Info VARCHAR(256) Artist Genre TRACK Column Data Type URI VARCHAR(256) Track Name VARCHAR(256) Artist Uri extension to be used for dest files. GENRE MOOD ","date":"2022-05-23","objectID":"/project2-2.-songplz-bot/:2:0","tags":null,"title":"[SongPlz-Bot] 2. Severless \u0026 Data Ingestion \u0026 Recommender System","uri":"/project2-2.-songplz-bot/"},{"categories":["Project"],"content":"SongPlz Bot - Slack and Spotify API ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:0:0","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"What is API API stands for Application Programming Interface. The most common example to describe the API is being a waiter in the restaurant. Interface between human and object Interface Interface is a program that allows a user to interact computers in person or over a network, which is a software intermediary that allows two applications to talk to each other Imagine you are sitting at the table in a Mexican restaurant, and you are thinking of ordering taco. You call a waiter to order a Taco Plate. Then, waiter takes a order and tells the kitchen that number 5 table customer ordered Taco Plate. Chef in the Kitchen cooks the Taco Plate. When the food is ready, waiter delivers the food to your table ! In this example, the kitchen is the part of the system that will prepare your order (user's request), and the waiter is the API that takes your request or order and tells the kitchen (system) what to do and delivers the food (response) to you. It can be very dangerous to directly communicate between the system and the user. APIs provides a layer of security so that your system is not fully exposed. The modern API adhere to standards (typically HTTP and REST), that are developer-friendly, easily accesssible and understood broadly. ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:1:0","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"HTTP API An HTTP API is an API that uses Hypertext Transfer Protocol as the communication protocol between the two systems (Educative) Protocol Protocol in computer science, a set of rules or procedures for transmitting data between electronic devices, such as computers. So, for the web, the protocol gives a rule about how a client (web browser) and a server (web server e.g. udacity.com) talk (Udacity). ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:1:1","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"REST API REST API stands for Representational State Transfer and is an architectural pattern for creating web services. It focuses on representational with URI and define state with HTTP Method. It’s essential that REST API’s design style strictly has to satisfy CRUD functions only. REST applications use HTTP methods like GET, POST, DELETE, and PUT (Educative). CRUD CRUD (CREATE, READ, UPDATE, DELETE) is used for anlything related to database and database design. OPERATION FUNCTION Create Insert Read Select Update Edit Delete Delete ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:1:2","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"Slack API Click here to create a bot for your slack workspace ! After adding your bot into your slack workspace, you can send a message as a Bot in python. There are few options to send messages through Slack API as a Bot ! ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:2:0","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"Option 1. import config import requests from datetime import datetime import logging from slack_sdk.errors import SlackApiError logger = logging.getLogger(__name__) SLACK_BOT_TEXT = ( \"HEY THERE, I FOUND A TOP 10 SONGS YOU MAY BE INTERESTED IN.\" \"THEY ARE LISTED BELOW\" ) class SlackFacade(object): def __init__(self, token = config.SLACK_BOT_TOKEN, bot_name = config.SLACK_BOT_NAME): self.token = token self.default_channel = config.DEFAULT_SLACK_CHANNEL self.bot_name = bot_name def emit(self, channel, text): \"\"\"Sends a message to your channel Args: channel ([String]): Specify the name of the slack channel \"\"\" try: text_with_date = datetime.now().strftime('[%m/%d%H:%M:%S] ') + text response = requests.post( \"https://slack.com/api/chat.postMessage\", headers={\"Authorization\": \"Bearer \"+ self.token}, data={\"channel\": channel,\"text\": text_with_date} ) except SlackApiError as e: logging.error(f\"Slack encountered error: {e.response['error']}\") raise e if __name__==\"__main__\": SlackFacade().emit( channel=config.DEFAULT_SLACK_CHANNEL, text=SLACK_BOT_TEXT ) Then, your channel will recieve a message from your app like this : Message from a SongPlz bot in Slack (Option 1) ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:2:1","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"Option 2. Code from slack_sdk import WebClient from slack_sdk.errors import SlackApiError block = { \"type\": \"section\", \"text\": { \"type\": \"mrkdwn\", \"text\": \"Hello :heart: this is *SongPlz*, Do you want *Global Top 10 Song*?\" } } class SlackFacade(object): def __init__(self, token=config.SLACK_BOT_TOKEN, bot_name=config.SLACK_BOT_NAME): self.token = token self.default_channel = config.DEFAULT_SLACK_CHANNEL self.bot_name = bot_name # Internally set properites self.client = WebClient(token=self.token) self.slack_message_entries = {} def emit(self, blocks, channel): \"\"\"Sends a message to your channel. Args: blocks: Expected to be a json-like array object containing the rich text representation of the listings we've found. The methods in this object should be used to construct the Rich Message Blocks. See the Slack kit builder for more information on block construction: https://app.slack.com/block-kit-builder channel: string, The channel to send the message to. \"\"\" try: response = self.client.chat_postMessage( channel=channel, text=\"SongPlz Incoming!\", blocks=blocks, # as_user must be false to set bot name username=self.bot_name ) except SlackApiError as e: logging.error(f\"Slack encountered an error: {e.response['error']}\") raise e return response SlackFacade().emit(blocks = [block], channel=config.DEFAULT_SLACK_CHANNEL) Message from a SongPlz bot in Slack (Option 2) Error (Solved) At first, I had so much struggles to fix the error encountered while tyring option 2. # error I received urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate I tried following two methods to fix the error. Method 1. (-\u003e this one failed for my case) pipenv install certifi # or pip install certifi Method 2. (if method 1 doesn’t work) import ssl ssl._create_default_https_context = ssl._create_unverified_context ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:2:2","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"Spotify API We can use Spotify’s Web API to discover music and podcasts. Let’s try Search API for items, you can get Spotify catalog information about albums, aritist, playlists, tracks, shows, or episodes that math a keyword string. Search API Request: GET /search HEADER: Authorization -\u003e Authorized Access_token is requred. q [string]: You can narrow down your search using field filters. album, artist, track, year, upc, tag:hipster, tag:new, isrc, genre. e.g. `remaster%20track:Doxy+artist:Miles%20Davis\" The artist filter can be used while searching albums, artists or tracks. The album and year filters can be used while searching albums or tracks. You can filter on a single year or a range (e.g. 1955-1960). The genre filter can be use while searching tracks and artists. The isrc and track filters can be used while searching tracks. The upc, tag:new and tag:hipster filters can only be used while searching albums. The tag:new filter will return albums released in the past two weeks and tag:hipster can be used to return only albums with the lowest 10% popularity. type [array]: A comma-separated list of item types to search across e.g. q=name:abacab\u0026type=album,track meaning it will return both albums and tracks with “abacab” included in their name limit [int]: default value is 20 market [string]: if a country code is specified, only content that is available in that market will be returned. e.g. “ES” offset [int]: The index of the first result to return. Reponse: 200 Body - application/json Tracks artist albums playlists shows episodes Each object contains href [string] items limit next offset previous total : total number of items available to return Example: BTS Song We need these to use API ! endpoint: https://api.spotify.com/v1/search method: GET parameter: {'q': 'BTS', 'type':'album', 'limit': 5} HEADER: {'Authorization': 'Bearer {}'.format(token)} This is just one of options to get a data using API, there are more ways to get a data! sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id=config.SPOTIFY_CLEINT_ID, client_secret=config.SPOTIFY_CLIENT_SECRET)) results = sp.search(q='BTS', limit=5) print(results) for idx, track in enumerate(results['tracks']['items']): print(idx, track['name']) ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:3:0","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"Concern for Better Data Pipeline We successfully retrieved information by using Spotify API. But, we have to think HOW to extract data more effectively. Here are still remaining questions to answer to get better data extraction. What if there is API restriction ? What if there is an error occurred in middle of the process? what if API authroization token is expired ? ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:4:0","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Project"],"content":"Reference https://www.educative.io/blog/what-are-rest-apis https://restfulapi.net/ https://www.youtube.com/watch?v=0U31-O4oEPc ","date":"2022-05-22","objectID":"/project2-1.-songplz-bot/:5:0","tags":null,"title":"[SongPlz-Bot] 1. Slack and Spotify API","uri":"/project2-1.-songplz-bot/"},{"categories":["Data Engineer"],"content":" Kafka with Docker In this post, we would like to go over how to run Kafka with Docker and Python. Before starting the post, make sure you have installed Docker (Docker hub) on your computer. ","date":"2022-05-21","objectID":"/running-kafka-docker/:0:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Step 1. Docker Image Setup Okay, first, let’s create a directory folder to store docker-compose.yml file. The docker-compose file does not run your code itself. $ mkdir ~/docker-kafka \u0026\u0026 cd docker-kafka You can pull kafka and zookeeper images by using this docker pull command, more detailed explanation can be found in the following link - kafka and zookeeper from Docker Hub. # Kafka $ docker pull wurstmeister/kafka # Zookeeper $ docker pull wurstmeister/zookeeper ","date":"2022-05-21","objectID":"/running-kafka-docker/:1:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Step 2. Create docker-compose.yml file ","date":"2022-05-21","objectID":"/running-kafka-docker/:2:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Docker Instead of pulling images separately, you can write docker-compose.yml file to pull those simultaneously. What is docker-compose.yml file? It is basically a config file for Docker Compose. It allows you to deploy, combine, and configure multiple docker containers at the same time. Is there difference between dockerfile and docker-compose? Yes! “A Dockerfile is a simple text file that contains the commands a user could call to assemble an image whereas Docker Compose is a tool for defining and running multi-container Docker applications” (dockerlab) ","date":"2022-05-21","objectID":"/running-kafka-docker/:2:1","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Docker-compose.yml file # This docker-compose file starts and runs:# * A 1-node kafka cluster# * A 1-zookeeper ensembleversion:'2'services:zookeeper:image:wurstmeister/zookeepercontainer_name:zookeeperports:- \"2181:2181\"kafka:image:wurstmeister/kafkacontainer_name:kafkaports:- \"9092:9092\"environment:KAFKA_ADVERTISED_HOST_NAME:127.0.0.1KAFKA_ZOOKEEPER_CONNECT:zookeeper:2181volumes:- /var/run/docker.sock:/var/run/docker.sock ","date":"2022-05-21","objectID":"/running-kafka-docker/:2:2","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Step 3. Run docker-compose Make sure you run the following command where docker-compose.yml file is located at. $ docker-compose up -d ","date":"2022-05-21","objectID":"/running-kafka-docker/:3:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Step 4. Run Kafka ! ","date":"2022-05-21","objectID":"/running-kafka-docker/:4:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"[Option 1] Execute docker container (bash) $ docker container exec -it kafka bash bash script will prompt! cd opt/kafka_2.13-2.8.1/bin # topic list kafka-topics.sh --list --zookeeper zookeeper:2181 # create topic kafka-console-producer.sh --bootstrap-server kafka:9092 --topic \u003ctopic-name\u003e # read events kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic \u003ctopic-name\u003e --from-beginning # delete topic kafka-topics.sh --zookeeper zookeeper:2181 --delete --topic \u003ctopic-name\u003e ","date":"2022-05-21","objectID":"/running-kafka-docker/:4:1","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"[Option 2] Access Kafka directly through command line # Check topic list $ docker exec -it kafka kafka-topics.sh --bootstrap-server kafka:9092 --list # Create topic $ docker exec -it kafka kafka-console-producer.sh --bootstrap-server kafka:9092 --topic \u003ctopic-name\u003e # Read events $ docker exec -it kafka kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic \u003ctopic-name\u003e --from-beginning ","date":"2022-05-21","objectID":"/running-kafka-docker/:4:2","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Check Your Environment Status You may run the following command at any time from a separate terminal instance: $ docker-compose ps ","date":"2022-05-21","objectID":"/running-kafka-docker/:5:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Stopping \u0026 Cleaning Up Docker Compose When you are ready to stop Docker compose you can run the following command $ docker-compose stop And if you’d like to clean up the container to reclaim disk space, as well as the columns containing your data, run the following command: $ docker-compose rm -v Are you sure? [yN] y ","date":"2022-05-21","objectID":"/running-kafka-docker/:6:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Further More with Python So, when you completed connecting kafka and docker, it’s time to actually get real-time tweets from twitter through kafka. Imagine, you own a small company which produces a service to users through own online platform. Then, there should be a source system like clickstream and a target system like own online platform. Data integration between the source system and target system woudln’t be that complicated. But, once the size of your company grows, the company would face lots of struggles when the company has more source systems and target systems with all different data sources. That’s the when the Kafka comes in. Kafka is a platform to get produced data from the source systems and the target systems read a streaming data from Kafka. Kafka Diagram The image is originally from a post explaining about Kakfa. I recommend the post ! In this post, we will create three files under src folder. ","date":"2022-05-21","objectID":"/running-kafka-docker/:7:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"1. credential.json Get Twitter API Credentials thorugh the link - TwitterAPI for Developer ","date":"2022-05-21","objectID":"/running-kafka-docker/:7:1","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"2. producer.py #src/producer.py BROKER_URL = \"localhost:9092\" TOPIC_NAME = \"\u003cyour topic_name\u003e\" ### twitter import tweepy from tweepy.auth import OAuthHandler from tweepy import Stream from tweepy.streaming import StreamListener import json import logging ### logging FORMAT = \"%(asctime)s| %(name)s- %(levelname)s- %(message)s\" LOG_FILEPATH = \"\u003cpath-to-your-log-file\u003e\" logging.basicConfig( filename=LOG_FILEPATH, level=logging.INFO, filemode='w', format=FORMAT) ### Authenticate to Twitter with open('src/credential.json','r') as f: credential = json.load(f) CONSUMER_KEY = credential['twitter_api_key'] CONSUMER_SECRET = credential['twitter_api_secret_key'] ACCESS_TOKEN = credential['twitter_access_token'] ACCESS_TOKEN_SECRET = credential['twitter_access_token_secret'] BEARER_TOKEN = credential['bearer_token'] from tweepy.streaming import StreamListener from tweepy import OAuthHandler from tweepy import Stream from kafka import KafkaProducer producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: v.encode('utf-8')) #Same port as your Kafka server class twitterAuth(): \"\"\"SET UP TWITTER AUTHENTICATION\"\"\" def authenticateTwitterApp(self): auth = OAuthHandler(consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET) auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET) return auth class TwitterStreamer(): \"\"\"SET UP STREAMER\"\"\" def __init__(self): self.twitterAuth = twitterAuth() def stream_tweets(self): while True: listener = ListenerTS() auth = self.twitterAuth.authenticateTwitterApp() stream = Stream(auth, listener) stream.filter(track=[\"Starbucks\"], stall_warnings=True, languages= [\"en\"]) class ListenerTS(StreamListener): def on_status(self, status): tweet = json.dumps({ 'id': status.id, 'name': status.user.name, 'user_location':status.user.location, 'text': status.text, 'fav': status.favorite_count, 'tweet_date': status.created_at.strftime(\"%Y-%m-%d%H:%M:%S\"), 'tweet_location': status.place.full_name if status.place else None }, default=str) producer.send(topic_name, tweet) return True if __name__ == \"__main__\": TS = TwitterStreamer() TS.stream_tweets() ","date":"2022-05-21","objectID":"/running-kafka-docker/:7:2","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"3. consumer.py from kafka import KafkaConsumer import json topic_name = \"\u003cyour_topic_name\u003e\" consumer = KafkaConsumer( topic_name, bootstrap_servers=['localhost:9092'], auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms = 5000, fetch_max_bytes = 128, max_poll_records = 100, value_deserializer=lambda x: json.loads(x.decode('utf-8'))) for message in consumer: tweets = json.loads(json.dumps(message.value)) print(tweets) ","date":"2022-05-21","objectID":"/running-kafka-docker/:7:3","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"4. Run producer.py and consumer.py files Open two different terminals. terminal 1 python src/consumer.py terminal 2 python src/producer.py The source code can be checked here in github ","date":"2022-05-21","objectID":"/running-kafka-docker/:7:4","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Data Engineer"],"content":"Reference image https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e ","date":"2022-05-21","objectID":"/running-kafka-docker/:8:0","tags":["kafka","docker"],"title":"[Kafka] Running Kafka with Docker (python)","uri":"/running-kafka-docker/"},{"categories":["Project"],"content":"SongPlz Bot - Architecture Planning ","date":"2022-05-21","objectID":"/project2-0.-songplz-bot/:0:0","tags":null,"title":"[SongPlz-Bot] 0. Slack Serverless Song Recommendataion Chatbot (SongPlz) - Architecture Planning","uri":"/project2-0.-songplz-bot/"},{"categories":["Project"],"content":"1. Aim The aim of this SongPlz Bot project (Slack Song Recommendation Bot) is to build a data pipeline to create a chatting bot for song recommendation with Serverless architecture which is a way to develop and run applications and services without managing infrastructure (AWS). If you are still unsure about serverless architecture, please click here. There are many cloud services available providing FaaS such as AWS Lambda, Azure Functions, and Google Cloud Function. In this project, AWS Serverless Architecture, aws Lambda, and api gateaway, will be used. Since it’s a project to perform a data pipeline, I would try to ingest data with different types of databases and data sources. A messenger is Slack. Mainly, AWS Database service will be used for the whole process. Expected Architecture planning is going to be like this: Architecture Planning ","date":"2022-05-21","objectID":"/project2-0.-songplz-bot/:1:0","tags":null,"title":"[SongPlz-Bot] 0. Slack Serverless Song Recommendataion Chatbot (SongPlz) - Architecture Planning","uri":"/project2-0.-songplz-bot/"},{"categories":["Project"],"content":"2. Happy Path Use the config file to get a set of queries we want to use for our artist search. Iterate over query objects and call Spotify API to get the songs of the artist. And recommend similar songs from different artists (son recommendation modeling is required at this stage) Format Slack messages according to preferred style/layout. Send Slack messages to the specified channel. Script Planning Create two folders - SongPlz, tests Under SongPlz folder config.py spotify_api_helper.py query_helper.py slack.py runner.py ","date":"2022-05-21","objectID":"/project2-0.-songplz-bot/:2:0","tags":null,"title":"[SongPlz-Bot] 0. Slack Serverless Song Recommendataion Chatbot (SongPlz) - Architecture Planning","uri":"/project2-0.-songplz-bot/"},{"categories":["Project"],"content":"3. Tech Stack Slack API Spotify API AWS account ","date":"2022-05-21","objectID":"/project2-0.-songplz-bot/:3:0","tags":null,"title":"[SongPlz-Bot] 0. Slack Serverless Song Recommendataion Chatbot (SongPlz) - Architecture Planning","uri":"/project2-0.-songplz-bot/"},{"categories":["Data Engineer"],"content":"Udacity Program Kafka (figure) Technologies used Apache Kafka, Kafka Connect, KSQL, Faust Stream Processing, Spark Structured Streaming ","date":"2022-05-19","objectID":"/udacity-dataengineer-datastreaming/:0:0","tags":["kafka"],"title":"[Udacity] Data Streaming","uri":"/udacity-dataengineer-datastreaming/"},{"categories":["Data Engineer"],"content":"About The Nanodegree Data Streaming skill was gained to be prepared for the next era of data engineering. Learned how to analyze data in real-time using Apache Kafka and Spark, and build applications to process live insights from data at scale. ","date":"2022-05-19","objectID":"/udacity-dataengineer-datastreaming/:1:0","tags":["kafka"],"title":"[Udacity] Data Streaming","uri":"/udacity-dataengineer-datastreaming/"},{"categories":["Data Engineer"],"content":"Program Details During the program, we completed two courses with two projects. Each course had different aims and technologies used. ","date":"2022-05-19","objectID":"/udacity-dataengineer-datastreaming/:1:1","tags":["kafka"],"title":"[Udacity] Data Streaming","uri":"/udacity-dataengineer-datastreaming/"},{"categories":["Data Engineer"],"content":"Course 1. Foundations of Data Streaming This course aims to learn the fundamentals of stream processing, including how to work with the Apache Kafka ecosystem, data schemas, ApacheAvro, Kafka connects and REST proxy, KSQL, and Faust Streaming Process. Associated exercise files and code can be found here ","date":"2022-05-19","objectID":"/udacity-dataengineer-datastreaming/:2:0","tags":["kafka"],"title":"[Udacity] Data Streaming","uri":"/udacity-dataengineer-datastreaming/"},{"categories":["Data Engineer"],"content":"Project 1. Optimize Chicago Public Transit transporation ","date":"2022-05-19","objectID":"/udacity-dataengineer-datastreaming/:2:1","tags":["kafka"],"title":"[Udacity] Data Streaming","uri":"/udacity-dataengineer-datastreaming/"},{"categories":["Data Engineer"],"content":"Course 2. Streaming API Development and Documentation In this course, the goal is to grow expertise in the components of streaming data systems and build a real-time analytics application. After this course, you will be able to identify Spark streaming (architecture and API) components, consume and process data from Apache Kafka with Spark Structured Streaming (including setting up and running a Spark Cluster), create a DataFrame as an aggregation of Source DataFrames, sink a composite DataFrame to Kafka, and visually inspect a data sink of accuracy. The relevant codes for the listed description can be found here ","date":"2022-05-19","objectID":"/udacity-dataengineer-datastreaming/:3:0","tags":["kafka"],"title":"[Udacity] Data Streaming","uri":"/udacity-dataengineer-datastreaming/"},{"categories":["Data Engineer"],"content":"Project 2. Evaluate Human Balance ","date":"2022-05-19","objectID":"/udacity-dataengineer-datastreaming/:3:1","tags":["kafka"],"title":"[Udacity] Data Streaming","uri":"/udacity-dataengineer-datastreaming/"},{"categories":["Machine Learning"],"content":"Udacity Program MLOps I participated Machine Learning DevOps Engineer program in Udacity (Certification). The program was extraordinary great, providing handful hands-on projects with the detailed and accurate human feedback. MLOps was whole different world and opened my eyes to the world of DevOps and machine learning in production level. The program is consisted of four projects, and each project covered different technology skills and tools. The most important thing the program emphasized was clean code principes, such as refactoring, handling errors, unit-testing, logging, and addressing. ","date":"2022-05-19","objectID":"/udacity-mlops/:0:0","tags":["mlops"],"title":"[Udacity] MLOps","uri":"/udacity-mlops/"},{"categories":["Machine Learning"],"content":"Projects Predicting Customer Churn Airbnb Rent prices in NYC Income prediction based on Census data Dynamic Risk Assessment System ","date":"2022-05-19","objectID":"/udacity-mlops/:1:0","tags":["mlops"],"title":"[Udacity] MLOps","uri":"/udacity-mlops/"},{"categories":["Machine Learning"],"content":"1. Predicting Customer Churn (link) We identified and predicted credit card customers that are most likely to churn, the percentage of service subscribers who discontinue their subscriptions within a given time period. The main objective of the project was to build a customer churn prediction model and implement all the clean code principles. Process EDA Feature Engineering (including encoding of categorical variables) Model Training Prediction Model Validation Technology Skills/Tools Pytest Pylint AutoPEP8 Logging and Error Handling ","date":"2022-05-19","objectID":"/udacity-mlops/:1:1","tags":["mlops"],"title":"[Udacity] MLOps","uri":"/udacity-mlops/"},{"categories":["Machine Learning"],"content":"2. Airbnb Rent Prices in NYC (link) In this project, we built machine learning pipeline by using MLflow and Wandb. We estimated the typical prices for a given property based on the price of similar properties. The company receives new data in bulk every week. The model needed to be retrained with the same cadence, necessitating an end-to-end pipeline that can be reused. Process Create Conda Environment Logging into Weights \u0026 Biases (API key, authorization) Use Cookiecutter Tool for the creation of steps required by the ML pipeline EDA Data Cleaning Data Testing Data Splitting Train Model (Random Forest) Hyperparameters Tuning Best Model Selection Testing Visualize the pipeline Release the pipeline Train the model on a new data sample Release on Github Technology Skills/Tools Weights \u0026 Biases MLflow Cookiecutter ","date":"2022-05-19","objectID":"/udacity-mlops/:1:2","tags":["mlops"],"title":"[Udacity] MLOps","uri":"/udacity-mlops/"},{"categories":["Machine Learning"],"content":"3. Income prediction based on Census data (link) We created and developed income prediction classification model on Census Bureau data by monitoring the model performance. Lastly, we deployed the model using the FastAPI on Heroku and created API tests for it. This whole procedure was incorporated into a CI/CD framework using Github Actions. There were many takeaways in this project, (1) model performance testing, (2) Data Version Control (DVC) as a remote storage and pipeline functionality performance (creating and running pipelines on dvc), (3) storing model in AWS S3 Bucket, (4) FastAPI, (5) deploying machine learning model on Heroku, (6) Github Actions. This project was the most difficult but the most enoyable one. Process Initialize Git and DVC and Connect Git repository to Github Set up AWS S3 Bucket from the AWS CLI ( create IAM user with the appropriate permissions) here Set up Github Actions - pytest, flaske8, python version, AWS credentials, DVC Commiting raw and transformed data on DVC remote storage pointing to the S3 Bucket. Train Model Unit Testing the trainig model Creating a RESTful API using FastAPI API Deployment on Heroku Technology Skills/Tools CI/CD DVC AWS S3 Bucket FastAPI Github Action Heroku ","date":"2022-05-19","objectID":"/udacity-mlops/:1:3","tags":["mlops"],"title":"[Udacity] MLOps","uri":"/udacity-mlops/"},{"categories":["Machine Learning"],"content":"4. Dynamic Risk Assessment System (link) We created, deployed, and monitored a risk assessment ML model that will estimate the attrition risk of each of the company’s 10,000 clients. The created API will return the model predictions, f1 score, summary statistics, and model diagnostics data (model metrics). Based on the model we created and deployed, the client managers will contact the clients with the highest risk and avoid losing clients and revenue. This entire process is automated and Process Data Ingestion Training, Scoring and Deploying an ML Model Diagnostics - model predictions, summary statistics, check missing data, performance timing, update dependencies. Create Model Reporting Process Automation - the model re-deployment Croning job for the full pipeline - run the fullprocess.py script one time every 10 min. Technology Skills/Tools ML Pipeline Automation Cronjob Flask ","date":"2022-05-19","objectID":"/udacity-mlops/:1:4","tags":["mlops"],"title":"[Udacity] MLOps","uri":"/udacity-mlops/"},{"categories":["Data Engineer"],"content":"Data Engineer Roadmap The roadmap aims to give a complete picture of the modern data engineering landscape and serve as a study guide for aspiring data engineers. Based on the provided Data Engineer Roadmap, I would like to dig into what we need to know to be a data engineer. Data Engineer Roadmap ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:1:0","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"Section ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:0","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"1. CS fundamentals Basic terminal usage Data structures \u0026 algorithms APIs REST Structured vs unstructured data Linux CLI Vim Shell scripting Cronjobs OS How does the computer work? How does the Internet work? Git — Version control Math \u0026 statistics basics ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:1","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"2. Programming Language The Most popular programming languages are Python Java Scala R Especially, Python, Java, Scala are the main languages for Data Engineer since these languages support Spark for big data processing. Python and R are highly recommended for aspiring Data Scientist,Data Analyst. ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:2","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"3. Testing Understanding TDD is very important. Unit Test Integration Test Functional Test ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:3","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"4. Database Fundamentals SQL is very important. Understand the Entity-Relationship (ER) modelling and Normalization. How to design databases and model data are important as well. Understand scalling pattern. CAP theorem OLAP vs OLAP Horizontal vs Vertical Scaling RDB vs No SQL Normalization Dimental Modelling ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:4","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"5. Relaitonal Database (RDB) fa-star): MySQL PostgreSQL ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:5","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"6. Non-relational Database (No SQL) : Understand the difference between Document, Wide Column, Graph and key-value. Recommended to master one database from each category. There are different types of DB like Documnet DB, Key-Value style DB Understaning Pro and Cons of No SQL Key-Value (DynamoDB, Redis) Document (MongoDB, Elasticsearch) Wide Column (Cassandra, HBase) ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:6","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"7. Data Warehouse Snowflake AWS Redshift Google BigQuery Azure Synapse Analytics ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:7","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"8. Object Storage AWS S3 Azure Blob Storage Google Cloud Storage ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:8","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"9. Cluster Computing Fundamentals Modern data processing frameworks are based on Apache Hadoop and MapReduce. Understanding these will help you learn modern fraeworks faster. Big Data, Cluster Computing, Distributed Computing. Hadoop HDFS MapReduce Managed Hadoop Managed Hadoop - Azure Data Lake / Google Dataproc / Amazon EMR ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:9","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"10. Data Processing Batch - data build tool Bybrid - Batch + Streaming (Spark, Flink) Streaming (Kafka, storm) ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:10","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"11. Messaging Google PubSub Azure Service Bus Rabbit MQ ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:11","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"12. Workflow Scheduling Apache Airflow Google Composer ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:12","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"13. Monitoring Data Pipelines Datadog ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:13","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"14. Networking Protocols (HTTP, HTTPS, TCP, SSH, IP, DNS) Firewalls VPN VPC ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:14","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"15. Infrastructure as Code Docker - container Kubernetes - container orchestration CDK - Infrastrue provisioning Terraform - Infrastrue provisioning ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:15","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"16. CI/CD Github Actions Jenkinds ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:16","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"17. Identity and Access Management AWS IAM Active Directory Azure Active Directory ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:17","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"18. Data Security and Privacy Legal Compliance Encryption Key Management Data Governance \u0026 Integrity ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:2:18","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Data Engineer"],"content":"Reference https://github.com/datastacktv/data-engineer-roadmap ","date":"2022-05-16","objectID":"/data-engineer-roadmap/:3:0","tags":null,"title":"Data Engineer Roadmap","uri":"/data-engineer-roadmap/"},{"categories":["Project"],"content":"Public Transit Status with Apache Kafka In this project, we constructed a streaming event pipeline around Apache Kafka and its ecosystem. Using public data from the Chicago Transit Authority we constructed an event pipeline around Kafka that allows us to simulate and display the status of train lines in real time. When the project is complete, you will be able to monitor a website to watch trains move from station to station. Check the source code in here ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:0:0","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Prerequisites The following are required to complete this project: Docker Python 3.7 ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:1:0","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Description The Chicago Transit Authority (CTA) has asked us to develop a dashboard displaying system status for its commuters. We have decided to use Kafka and ecosystem tools like REST Proxy and Kafka Connect to accomplish this task. Our architecture will look like so: Project Diagram ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:0","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Step 1: Create Kafka Producers The first step in our plan is to configure the train stations to emit some of the events that we need. The CTA has placed a sensor on each side of every train station that can be programmed to take an action whenever a train arrives at the station. To accomplish this, you must complete the following tasks: Complete the code in producers/models/producer.py Define a value schema for the arrival event in producers/models/schemas/arrival_value.json with the following attributes station_id train_id direction line train_status prev_station_id prev_direction Complete the code in producers/models/station.py so that: A topic is created for each station in Kafka to track the arrival events The station emits an arrival event to Kafka whenever the Station.run() function is called. Ensure that events emitted to kafka are paired with the Avro key and value schemas Define a value schema for the turnstile event in producers/models/schemas/turnstile_value.json with the following attributes station_id station_name line Complete the code in producers/models/turnstile.py so that: A topic is created for each turnstile for each station in Kafka to track the turnstile events The station emits a turnstile event to Kafka whenever the Turnstile.run() function is called. Ensure that events emitted to kafka are paired with the Avro key and value schemas ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:1","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Step 2: Configure Kafka REST Proxy Producer Our partners at the CTA have asked that we also send weather readings into Kafka from their weather hardware. Unfortunately, this hardware is old and we cannot use the Python Client Library due to hardware restrictions. Instead, we are going to use HTTP REST to send the data to Kafka from the hardware using Kafka’s REST Proxy. To accomplish this, you must complete the following tasks: Define a value schema for the weather event in producers/models/schemas/weather_value.json with the following attributes temperature status Complete the code in producers/models/weather.py so that: A topic is created for weather events The weather model emits weather event to Kafka REST Proxy whenever the Weather.run() function is called. NOTE: When sending HTTP requests to Kafka REST Proxy, be careful to include the correct Content-Type. Pay close attention to the examples in the documentation for more information. Ensure that events emitted to REST Proxy are paired with the Avro key and value schemas ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:2","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Step 3: Configure Kafka Connect Finally, we need to extract station information from our PostgreSQL database into Kafka. We’ve decided to use the Kafka JDBC Source Connector. To accomplish this, you must complete the following tasks: Complete the code and configuration in producers/connectors.py Please refer to the Kafka Connect JDBC Source Connector Configuration Options for documentation on the options you must complete. You can run this file directly to test your connector, rather than running the entire simulation. Make sure to use the Landoop Kafka Connect UI and Landoop Kafka Topics UI to check the status and output of the Connector. To delete a misconfigured connector: CURL -X DELETE localhost:8083/connectors/stations ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:3","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Step 4: Configure the Faust Stream Processor We will leverage Faust Stream Processing to transform the raw Stations table that we ingested from Kafka Connect. The raw format from the database has more data than we need, and the line color information is not conveniently configured. To remediate this, we’re going to ingest data from our Kafka Connect topic, and transform the data. To accomplish this, you must complete the following tasks: Complete the code and configuration in `consumers/faust_stream.py Watch Out! You must run this Faust processing application with the following command: faust -A faust_stream worker -l info ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:4","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Step 5: Configure the KSQL Table Next, we will use KSQL to aggregate turnstile data for each of our stations. Recall that when we produced turnstile data, we simply emitted an event, not a count. What would make this data more useful would be to summarize it by station so that downstream applications always have an up-to-date count To accomplish this, you must complete the following tasks: Complete the queries in consumers/ksql.py Tips The KSQL CLI is the best place to build your queries. Try ksql in your workspace to enter the CLI. You can run this file on its own simply by running python ksql.py Made a mistake in table creation? DROP TABLE \u003cyour_table\u003e. If the CLI asks you to terminate a running query, you can TERMINATE \u003cquery_name\u003e ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:5","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Step 6: Create Kafka Consumers With all of the data in Kafka, our final task is to consume the data in the web server that is going to serve the transit status pages to our commuters. To accomplish this, you must complete the following tasks: Complete the code in consumers/consumer.py Complete the code in consumers/models/line.py Complete the code in consumers/models/weather.py Complete the code in consumers/models/station.py ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:6","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Documentation In addition to the course content you have already reviewed, you may find the following examples and documentation helpful in completing this assignment: Confluent Python Client Documentation Confluent Python Client Usage and Examples REST Proxy API Reference Kafka Connect JDBC Source Connector Configuration Options ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:2:7","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Directory Layout The project consists of two main directories, producers and consumers. The following directory layout indicates the files that the student is responsible for modifying by adding a * indicator. Instructions for what is required are present as comments in each file. * - Indicates that the files I completed the code in this file. Other files are prepared by Udacity beforehand. ├── consumers │ ├── consumer.py * │ ├── faust_stream.py * │ ├── ksql.py * │ ├── models │ │ ├── lines.py │ │ ├── line.py * │ │ ├── station.py * │ │ └── weather.py * │ ├── requirements.txt │ ├── server.py │ ├── topic_check.py │ └── templates │ └── status.html └── producers ├── connector.py * ├── models │ ├── line.py │ ├── producer.py * │ ├── schemas │ │ ├── arrival_key.json │ │ ├── arrival_value.json * │ │ ├── turnstile_key.json │ │ ├── turnstile_value.json * │ │ ├── weather_key.json │ │ └── weather_value.json * │ ├── station.py * │ ├── train.py │ ├── turnstile.py * │ ├── turnstile_hardware.py │ └── weather.py * ├── requirements.txt └── simulation.py ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:3:0","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Running and Testing To run the simulation, you must first start up the Kafka ecosystem on their machine utilizing Docker Compose. %\u003e docker-compose up ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:4:0","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Running the Simulation There are two pieces to the simulation, the producer and consumer. As you develop each piece of the code, it is recommended that you only run one piece of the project at a time. However, when you are ready to verify the end-to-end system prior to submission, it is critical that you open a terminal window for each piece and run them at the same time. If you do not run both the producer and consumer at the same time you will not be able to successfully complete the project. To run the producer: cd producers virtualenv venv . venv/bin/activate pip install -r requirements.txt python simulation.py Once the simulation is running, you may hit Ctrl+C at any time to exit. To run the Faust Stream Processing Application: cd consumers virtualenv venv . venv/bin/activate pip install -r requirements.txt faust -A faust_stream worker -l info To run the KSQL Creation Script: cd consumers virtualenv venv . venv/bin/activate pip install -r requirements.txt python ksql.py To run the consumer: NOTE: Do not run the consumer until you have reached Step 6! cd consumers virtualenv venv . venv/bin/activate pip install -r requirements.txt python server.py ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:4:1","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":["Project"],"content":"Whole Simulation Process kafka-topics --list --zookeeper localhost:2181 cd producers python connector.py cd ../ kafka-console-consumer --bootstrap-server localhost:9092 --topic postgre_connect_stations --from-beginning cd consumers faust -A faust_stream worker -l info cd ../ kafka-console-consumer --bootstrap-server localhost:9092 --topic org.chicago.cta.weather.v1 --from-beginning cd producers python simulation.py cd consumers python server.py # delete topics kafka-topics --zookeeper localhost:2181 --delete --topic arrival.station.* kafka-topics --zookeeper localhost:2181 --delete --topic turnstile_station kafka-topics --zookeeper localhost:2181 --delete --topic org.chicago.cta.weather.v1 ","date":"2022-04-10","objectID":"/project3-chicago-optimizing-public-transportation/:4:2","tags":null,"title":"[Chicago Optimizng Public Transportation]","uri":"/project3-chicago-optimizing-public-transportation/"},{"categories":null,"content":"Setting up the virtual environment is especially crucial for machine learning projects (data science projects) because of the number of package modules and different versions. There are three ways of creating a virtual environment setup in Python. In this post, three different ways of virtual environments will be introduced step by step, conda, pip, venv! I prefer using conda and venv. Python Virtual Environment ","date":"2022-01-10","objectID":"/virtual_env_setup/:0:0","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"Conda Environment Set up Download and install conda if you don’t have it already. ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:0","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"1. Create get.sh # get.sh wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:1","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"2. Run get.sh file \u0026 Miniconda3-latest-Linux-x86_64.sh # got checking contents of get.sh file cat get.sh bash get.sh bash Miniconda3-latest-Linux-x86_64.sh Type ‘yes’ ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:2","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"3. Then set the workspace path {pwd}/miniconda3 Make sure to set the path of workplace after checking current working directory in another terminal using pwd command ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:3","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"4. Modify .gitgnore file @ .gitgnore file # conda for gitpod env anaconda3 miniconda3 After pushing it to git, remove two files - (1)get.sh, (2)Miniconda3-latest-Linux-x86_64.sh ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:4","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"5. Create Environment [Option 1] Then create a new environment using code and activate it: conda create -n \u003cenvironment_name\u003e \"python=3.8\" scikit-learn dvc pandas numpy pytest jupyter jupyterlab fastapi uvicorn -c conda-forge dvc-s3 sklearn conda activate \u003cenvironment_name\u003e [Option 2] create a new environment using the environment.ymlfile provided in the root of the repository and activate it: name:\u003cenvironment_name\u003echannels:- conda-forge- defaultsdependencies:- python=3.8- pip- pandas- matplotlib- scikit-learn- pip:- lightgbm conda env create -f environment.yml conda activate \u003cenvironment_name\u003e If you haven’t installed git, please install git through conda conda install git ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:5","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"6. Packaging a conda environment with conda-pack # install conda-pack conda install -c conda-forge conda-pack # Activate the desired environment conda activate environment_name # set the path to desired directory to save the package cd path/to/desired/directory. # pack your environment (it might take few minutes) conda pack # environment is packaged as a tar.gz file ls ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:6","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"7. Unpackaging a conda environment with conda-pack # create a folder for the environment mkdir \u003cenvironment_folder\u003e # Uncompress the environment in the folder tar xzvf \u003cenvironment_name\u003e.tar.gz -C \u003cenvironment_folder\u003e # check the folder ls # Activate the environment source \u003cenvironment_name\u003e/bin/activate # Deactivate the environment source \u003cenvironment_name\u003e/bin/deactivate ","date":"2022-01-10","objectID":"/virtual_env_setup/:1:7","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"pip Environment Set up ","date":"2022-01-10","objectID":"/virtual_env_setup/:2:0","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"1. Create a virtual environment python -m \u003cvirtual environment name\u003e ","date":"2022-01-10","objectID":"/virtual_env_setup/:2:1","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"2. Activate this virtual environment source \u003cvirtual environment name\u003e/bin/activate after doing this, you may notice the prompt adds the name of the virtual environment (\u003cvirtual environment name\u003e)$ ","date":"2022-01-10","objectID":"/virtual_env_setup/:2:2","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"3. Installing list of packages in requirements.txt pip install -r requirements.txt ","date":"2022-01-10","objectID":"/virtual_env_setup/:2:3","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"4. Useful command show a list of all installed python modules pip freeze show a list containing only the outdated modules pip list —outdated provides a list of installed modules in “requirements.format” pip list provide specific information about the pandas module pip show pandas install the pandas module pip install pandas run pip using python python -m pip list ","date":"2022-01-10","objectID":"/virtual_env_setup/:2:4","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"venv Environment Set up ","date":"2022-01-10","objectID":"/virtual_env_setup/:3:0","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"1. Create a virtual environment python -m venv \u003cname-of-your-new-virtual-environment\u003e ","date":"2022-01-10","objectID":"/virtual_env_setup/:3:1","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"2. Activate this virtual environment source \u003cname-of-your-new-virtual-environment\u003e/bin/activate After doing this, you may notice the prompt adds the name of the virtual environment (\u003cname-of-your-new-virtual-environment\u003e)$ ","date":"2022-01-10","objectID":"/virtual_env_setup/:3:2","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"3. Installing list of packages in requirements.txt pip install -r requirements.txt ","date":"2022-01-10","objectID":"/virtual_env_setup/:3:3","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"4. Useful command show a list of all installed python modules and create requirements.txt pip3 freeze \u003e requirements.txt show a list containing only the outdated modules pip3 list —outdated provides a list of installed modules in “requirements.format” pip3 list provide specific information about the pandas module pip3 show pandas install the pandas module pip3 install pandas deactivate the environment and delete the envrionment deactivate rm -rf \u003cname-of-your-new-virtual-environment\u003e ","date":"2022-01-10","objectID":"/virtual_env_setup/:3:4","tags":null,"title":"[Python] Three different Virtual Environments Setup (Mac) - Step By Step (conda, pip, venv)","uri":"/virtual_env_setup/"},{"categories":null,"content":"About","date":"2019-08-02","objectID":"/about/","tags":null,"title":"About Myself","uri":"/about/"},{"categories":null,"content":"  Hello! my name is Youhee Kil who constantly learning to be a better data developer. I used to work as a project manager and data analyst intern in Los Angeles. I recently obtained a Master’s Degree in Statistics and Data Science from KU LEUVEN, Belgium. Currently, I am learning to be a better AI/Machine Learning Engineer with Backend Knowledge! If you’d like to know more about me, please click HERE. Youhee Kil ","date":"2019-08-02","objectID":"/about/:0:0","tags":null,"title":"About Myself","uri":"/about/"}]